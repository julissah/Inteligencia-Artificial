{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RN_Perceptron_multicapa.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXMXd8gWpSboKJYeC3CbYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julissah/Inteligencia-Artificial/blob/main/RN_Perceptron_multicapa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrAJzGucec0-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv \n",
        "import math\n",
        "import requests, io\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbK5ysoAtKIG"
      },
      "source": [
        "db = 'titanic'\n",
        "n = 10\n",
        "train = 0\n",
        "test = 0\n",
        "txt = open (\"DataSet-\"+db+'-Resumen.txt','w')\n",
        "txt.write('Base de datos: '+db+'\\nDescripción:\\n\\n')\n",
        "txt.write('La presente base de datos muestra el conjunto de datos del Titanic proporciona los valores de cuatro atributos categóricos para cada una de las 2.201 personas que estaban a bordo del Titanic cuando chocó con un iceberg y se hundió. Los atributos son la clase social (primera clase, segunda clase, tercera clase, miembro de la tripulación), la edad (adulto o niño), el sexo y si la persona sobrevivió o no. \\n')\n",
        "txt.write('La cuestión de interés para este conjunto de datos naturales es cómo se relaciona la supervivencia con los demás atributos. \\n\\n')\n",
        "zipFileUrl = 'https://sci2s.ugr.es/keel/dataset/data/classification/'+db+'-'+str(n)+'-fold.zip'\n",
        "train_File = 'data/'+db+'-'+str(n)+'-fold/'+db+'-'+str(n)+'-tra.csv'\n",
        "result = requests.get(zipFileUrl)\n",
        "file_zip = ZipFile(io.BytesIO(result.content))\n",
        "file_zip.extractall('data/'+db+'-'+str(n)+'-fold')"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIncsJur3-5A"
      },
      "source": [
        "def trainWrite (open_File, train):\n",
        "    with open(open_File) as infile:\n",
        "      row = infile.read().splitlines()[11:]\n",
        "      for rw in row:\n",
        "        train +=1;\n",
        "        txt.write(str(rw.split(\",\"))+'\\n')\n",
        "        up_csv.writerow(rw.split(\",\"))"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNP46HhwtNel"
      },
      "source": [
        "txt.write('Datos entrenamiento:\\n')\n",
        "with open(train_File, \"w\") as outfile:\n",
        "  up_csv = csv.writer(outfile)\n",
        "  txt.write(str(['At1', 'At2', 'At3', 'Y'])+'\\n')\n",
        "  up_csv.writerow(['At1', 'At2', 'At3', 'Y'])\n",
        "  for i in range(1,(n+1)):\n",
        "    open_File = 'data/'+db+'-'+str(n)+'-fold/'+db+'-'+str(n)+'-'+str(i)+'tra.dat'\n",
        "    trainWrite(open_File, train)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ol4D8DJ4uYh"
      },
      "source": [
        "def testWrite(open_File, tst_File, test):\n",
        "    with open(open_File) as infile, open(tst_File, \"w\") as outfile:\n",
        "      row = infile.read().splitlines()[11:]\n",
        "      up_csv = csv.writer(outfile)\n",
        "      up_csv.writerow(['At1', 'At2','At3', 'Y'])\n",
        "      for rw in row:\n",
        "        test +=1\n",
        "        txt.write(str(rw.split(\",\"))+'\\n')\n",
        "        up_csv.writerow(rw.split(\",\"))"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWB6nDuTtQ_7"
      },
      "source": [
        "txt.write('Datos de prueba:\\n')\n",
        "txt.write(str(['At1', 'At2','At3', 'Y'])+'\\n')\n",
        "for i in range(1,(n+1)):\n",
        "  open_File = 'data/'+db+'-'+str(n)+'-fold/'+db+'-'+str(n)+'-'+str(i)+'tst.dat'\n",
        "  tst_File = 'data/'+db+'-'+str(n)+'-fold/'+db+'-'+str(n)+'-'+str(i)+'tst.csv'\n",
        "  testWrite(open_File, tst_File, test)  \n",
        "txt.close()"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unhva5XrtV7h",
        "outputId": "9d0fcd4a-be41-43fb-fed9-6a65658f9415"
      },
      "source": [
        "MLP = MLPClassifier(activation='relu',\n",
        "                    hidden_layer_sizes = (10, 5),\n",
        "                    learning_rate_init = 0.003,\n",
        "                    max_iter = 100000)\n",
        "\n",
        "dataBase = pd.read_csv(train_File)\n",
        "At1 = dataBase.loc[:, dataBase.columns != 'Y']\n",
        "Y = dataBase.Y\n",
        "MLP.fit(At1,Y)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(10, 5), learning_rate='constant',\n",
              "              learning_rate_init=0.003, max_fun=15000, max_iter=100000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqQ5xyyj9x1V"
      },
      "source": [
        "def accuracysCalc():\n",
        "  acuracyTest = 0\n",
        "  accuracys = [];\n",
        "  c1 =[]\n",
        "  c2 =[]\n",
        "\n",
        "  for i in range(1,(n+1)):\n",
        "    tst_File = 'data/'+db+'-'+str(n)+'-fold/'+db+'-'+str(n)+'-'+str(i)+'tst.csv'\n",
        "    dataBase_test = pd.read_csv(tst_File)\n",
        "    At1_test = dataBase_test.loc[:, dataBase.columns != 'Y']\n",
        "    Y_test = dataBase_test.Y\n",
        "    testPred = MLP.predict(At1_test)\n",
        "    accuracyTest = accuracy_score(testPred, Y_test)\n",
        "    acuracyTest += accuracyTest\n",
        "    accuracys.append(accuracyTest)\n",
        "    c1.append(str(i-1))\n",
        "    c2.append(str(round(accuracyTest*100,2))+'%')\n",
        "    txt.write('Fold '+str(i-1)+', Tasa de clasificación en el conjunto de datos de test.: '+str(round(accuracyTest*100,2))+'%\\n')\n",
        "\n",
        "  mean = acuracyTest/n\n",
        "  sum1 = 0\n",
        "  for a in accuracys:\n",
        "   sum1 += (a - mean)**2\n",
        "  desvStd = math.sqrt((1/(n-1))*(sum1))\n",
        "  c1.append('Media')\n",
        "  c1.append('Desviación Estandar')\n",
        "  c2.append(str(round(mean*100,2))+'%')\n",
        "  c2.append(str(desvStd))\n",
        "  txt.write('Media: '+str(round(mean*100,2))+'%\\n')\n",
        "  txt.write('Desviación Estandar: '+str(desvStd)+'\\n')\n",
        "  txt.close() \n",
        "  df = pd.DataFrame(list(zip(c1, c2)),\n",
        "               columns =['Fold', '%'])\n",
        "  print(df)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMqJHrHyv1_n",
        "outputId": "6deaa1b1-77c8-4ec8-993b-8bdd84be2239"
      },
      "source": [
        "txt = open ('PerceptronMulticapa.txt','w')\n",
        "txt.write('Red Neuronal Perceptrón Multicapa\\n\\n')\n",
        "txt.write('Alumna: Julissa Huaman Hilari\\n\\n')\n",
        "txt.write('Cantidad de elementos en el conjunto de datos de entrenamiento: '+str(train)+'\\n')\n",
        "txt.write('Cantidad de elementos en el conjunto de datos de test: '+str(test)+'\\n')\n",
        "txt.write('Cantidad de características: 3\\n')\n",
        "txt.write('Cantidad de clases: 2\\n')\n",
        "txt.write('Parámetros utilizados: 5\\n')\n",
        "txt.write('cantidad de capas ocultas: 10\\n')\n",
        "trainPred = MLP.predict(At1)\n",
        "accuracyTrain = accuracy_score(trainPred, Y)\n",
        "txt.write('Tasa de clasificación en el conjunto de datos de entrenamiento: '+str(round(accuracyTrain*100,2))+'%\\n')\n",
        "accuracysCalc()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Fold                     %\n",
            "0                     0                77.06%\n",
            "1                     1                79.72%\n",
            "2                     2                81.11%\n",
            "3                     3                81.11%\n",
            "4                     4                79.26%\n",
            "5                     5                75.12%\n",
            "6                     6                80.65%\n",
            "7                     7                79.72%\n",
            "8                     8                74.65%\n",
            "9                     9                81.57%\n",
            "10                Media                 79.0%\n",
            "11  Desviación Estandar  0.025167131876294803\n"
          ]
        }
      ]
    }
  ]
}